---
title: "Practical Homework I"
author: "Richa"
date: "2025-04-03"
output: html_document
---

#### In this project, we are going to apply our knowledge of Decision Trees. We will be modelling single trees, pruning them, using ensemble methods like bagging, boosting and randomForests to explore and analyse youth drug use. The data given is very large (10,561 rows!) and has a lot of variabes (79!). Since we will not be using all of the variables, cleaning and factorsing variables of our choice will be smarter than cleaning all columns of the dataset.

##### For this assignment, I have decided to focus on the question "IMPACT OF FAMILY BACKGROUND AND SOCIOECONOMIC STATUS ON MARIJUANA USE". We have around 15 variables of interest, which we will see throughout this assignment.

```{r}
#Importing Libraries
library(dplyr)
library(tidyr)
library(tree)
library(caret)
library(ggplot2)
library(randomForest) 
library(gbm)  

#Loading the Data
load("/Users/rich/Downloads/youth_data.Rdata")
View(df)

#Exploring the Data
ncol(df)
nrow(df)
str(df)
summary(df)
colnames(df)
View(df)
```


##### The Codebook about this data explained in-depth about certain code conventions present in the data. The code conventions had to be either converted to 0 or NA depending on their meaning.

```{r}
#Cleaning Data
#Substituting standard code conventions with NA
code_conventions <- function(x) {
  # Convert to numeric
  x <- as.numeric(x)
  
  # Codes for "NEVER USED [DRUG(s) OF INTEREST]" 
  never_used_codes <- c(91, 991, 9991, 81, 981, 9981)
  x[x %in% never_used_codes] <- 0  
  
  # Codes for "USED [DRUG] BUT NOT IN THE PERIOD OF INTEREST" 
  used_not_in_period_codes <- c(93, 993, 9993, 83, 983, 9983)
  x[x %in% used_not_in_period_codes] <- 0  
  
  # Codes for "DON’T KNOW" 
  dont_know_codes <- c(94, 994, 9994)
  x[x %in% dont_know_codes] <- NA  
  
  # Codes for "REFUSED" 
  refused_codes <- c(97, 997, 9997)
  x[x %in% refused_codes] <- NA 
  
  # Codes for "BLANK (not answered)" 
  blank_codes <- c(98, 998, 9998)
  x[x %in% blank_codes] <- NA 
  
  # Codes for "LEGITIMATE SKIP" 
  legitimate_skip_codes <- c(99, 999, 9999, 89, 989, 9989)
  x[x %in% legitimate_skip_codes] <- NA  
  
  # Codes for "BAD DATA" 
  bad_data_codes <- c(85, 985, 9985)
  x[x %in% bad_data_codes] <- NA 
  
  return(x)
}

columns <- c("EDUSKPCOM", "EDUSCHGRD2", "IRALCFY", "IRMJFY", "IRCIGFM", "IRSMKLSS30N", 
          "IRALCFM", "IRMJFM", "IRCIGAGE", "IRSMKLSSTRY", 
          "IRALCAGE", "IRMJAGE")

for (column in columns) {
  df[[column]] <- code_conventions(df[[column]])
}

head(df)
```


##### For Classification Decision Trees data interpretation would be easier if all the numeric factors such as 0 = NO and 1 = YES are transformed to classes such as NO and YES. Also there are variables like INCOME which have classes between 1 and 4, each number representing different ranges of income. Factorising these variables is also important.

```{r}
# Converting numeric factors to labels for better data interpretation.
#Binary Variables
df$MRJFLAG  <- factor(df$MRJFLAG,  levels = c(0, 1), labels = c("No", "Yes"))
df$ALCFLAG  <- factor(df$ALCFLAG,  levels = c(0, 1), labels = c("No", "Yes"))
df$TOBFLAG  <- factor(df$TOBFLAG,  levels = c(0, 1), labels = c("No", "Yes"))
df$PARCHKHW  <- factor(df$PARCHKHW,  levels = c(1, 2), labels = c("Yes", "No"))
df$PARHLPHW  <- factor(df$PARHLPHW,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRCHORE2  <- factor(df$PRCHORE2,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRLMTTV2  <- factor(df$PRLMTTV2,  levels = c(1, 2), labels = c("Yes", "No"))
df$PARLMTSN  <- factor(df$PARLMTSN,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRGDJOB2  <- factor(df$PRGDJOB2,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRPROUD2  <- factor(df$PRPROUD2,  levels = c(1, 2), labels = c("Yes", "No"))
df$ARGUPAR   <- factor(df$ARGUPAR,   levels = c(1, 2), labels = c("Yes", "No"))
df$PRPKCIG2  <- factor(df$PRPKCIG2,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRMJEVR2  <- factor(df$PRMJEVR2,  levels = c(1, 2), labels = c("Yes", "No"))
df$PRMJMO    <- factor(df$PRMJMO,    levels = c(1, 2), labels = c("Yes", "No"))
df$PRALDLY2  <- factor(df$PRALDLY2,  levels = c(1, 2), labels = c("Yes", "No"))
df$GOVTPROG  <- factor(df$GOVTPROG,  levels = c(1, 2), labels = c("Yes", "No"))

# Multi-category Variables
df$IMOTHER <- factor(df$IMOTHER, levels = c(1, 2, 3, 4),
                     labels = c("Yes", "No", "Don't know", "Over 18"))

df$IFATHER <- factor(df$IFATHER, levels = c(1, 2, 3, 4),
                     labels = c("Yes", "No", "Don't know", "Over 18"))

# Ordered Variables
df$INCOME <- factor(df$INCOME, levels = c(1, 2, 3, 4),
                    labels = c("Less than $20,000", "$20,000 - $49,999", 
                               "$50,000 - $74,999", "$75,000 or More"))

df$POVERTY3 <- factor(df$POVERTY3, ordered = TRUE, levels = c(1, 2, 3, 4),
                      labels = c("Below poverty level", "Up to 2X poverty threshold",
                                 "Up to 3X poverty threshold", "More than 3X threshold"))
```


##### Visualising some plots concerning our variables, so we know how are the different classes populated in different variables. 

```{r}
# Data Visualisation
# Plot INCOME vs ALCFLAG (Alcohol Use)
ggplot(df, aes(x = INCOME, fill = ALCFLAG)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "INCOME vs Alcohol Use", x = "Income Level", y = "Count") +
  scale_fill_manual(values = c("Yes" = "skyblue", "No" = "pink"))
```

```{r}
# Plot PARCHKHW vs TOBFLAG
ggplot(df, aes(x = PARCHKHW, fill = TOBFLAG)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "Parents Arguing vs Tobacco Use", x = "Parents Arguing (Yes/No)", y = "Count") +
  scale_fill_manual(values = c("Yes" = "grey", "No" = "blue"))
```

```{r}
ggplot(df, aes(x = PRCHORE2, fill = MRJFLAG)) +
  geom_bar(position = "dodge", stat = "count") +
  labs(title = "Doing Household Chores vs Tobacco Use", x = "Household Chores (Yes/No)", y = "Count") +
  scale_fill_manual(values = c("Yes" = "yellow", "No" = "black"))
```




```{r}
# POVERTY vs IRALCFY, IRMJFY
df_line <- df %>%
  group_by(POVERTY3) %>%
  summarise(
    Alcohol = mean(IRALCFY, na.rm = TRUE),
    Marijuana = mean(IRMJFY, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_longer(cols = c(Alcohol, Marijuana), names_to = "Substance", values_to = "MeanUsage")

ggplot(df_line, aes(x = POVERTY3, y = MeanUsage, color = Substance, group = Substance)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Mean Substance Use Frequency vs Poverty Level",
    x = "Poverty Level",
    y = "Mean Use Frequency"
  ) +
  theme_minimal()
```


```{r}
set.seed(123)

# Training and Testing Data Split
train_indices <- sample(1:nrow(df), size = 0.7 * nrow(df))

df_train <- df[train_indices, ]
df_test <- df[-train_indices, ]
```


#### BINARY CLASSIFICATION

```{r}
# Set seed for reproducibility
set.seed(123)

# Binary Tree Model
binary_tree <- tree(MRJFLAG ~ PARCHKHW + PARHLPHW + PRCHORE2 + 
                    PRLMTTV2 + PARLMTSN + PRGDJOB2 + PRPROUD2 + ARGUPAR + 
                    PRMJEVR2 + PRMJMO + IMOTHER + IFATHER,
                    data = df_train)

# Predictions on test data, confusion matrix and accuracy calculation
binary_pred <- predict(binary_tree, newdata = df_test, type = "class")
confusion_matrix <- table(Predicted = binary_pred, Actual = df_test$MRJFLAG)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy

# Tree PLot
plot(binary_tree)
text(binary_tree)

# Cross Validation and its plot
binary_cv <- cv.tree(binary_tree, FUN = prune.misclass)
plot(binary_cv$size, binary_cv$dev, type = "b",
     xlab = "Tree Size", ylab = "Misclassification Error",
     main = "CV Error vs. Tree Size")
```

##### CV error vs Tree Size is a straight line! This can mean that the distribution of data in both the classes (Yes and No) is very imbalanced. 

```{r}
# Class Imbalance Check
table(df_train$MRJFLAG)
prop.table(table(df_train$MRJFLAG))
```

##### We see that the Yes class has very few observations as opposed to the No class. We can upsample and downsample respectively to get some decent results. Let us try resampling.

##### We use mincut, minsize, mindev so that the tree is more complex. There is not much variance in the data so without the three factors we end up with a stump. (Used Internet to get these factors in the tree function).

```{r}
# Set seed for reproducibility
set.seed(123)

# Factorization
df_train$MRJFLAG <- as.factor(df_train$MRJFLAG)
df_test$MRJFLAG <- as.factor(df_test$MRJFLAG)

# Downsampling the training set
df_train_bal <- downSample(x = df_train[, -which(names(df_train) == "MRJFLAG")],
                           y = df_train$MRJFLAG,
                           yname = "MRJFLAG")

# Checking class balance for Yes and No classes
print(prop.table(table(df_train_bal$MRJFLAG)))

# Binary Tree
binary_tree1 <- tree(MRJFLAG ~ PARCHKHW + PARHLPHW + PRCHORE2 + 
                    PRLMTTV2 + PARLMTSN + PRGDJOB2 + PRPROUD2 + ARGUPAR + 
                    PRMJEVR2 + PRMJMO + IMOTHER + IFATHER,
                    data = df_train_bal, mincut = 7, minsize = 15, mindev = 0.001)

# Predictions on test data, confusion matrix and accuracy calculation
binary_pred1 <- predict(binary_tree1, newdata = df_test, type = "class")
confusion_matrix1 <- table(Predicted = binary_pred1, Actual = df_test$MRJFLAG)
accuracy1 <- sum(diag(confusion_matrix1)) / sum(confusion_matrix1)
accuracy1


# Cross Validation and its plot
binary_cv1 <- cv.tree(binary_tree1, FUN = prune.misclass)
plot(binary_cv1$size, binary_cv1$dev, type = "b",
     xlab = "Tree Size", ylab = "Misclassification Error",
     main = "CV Error vs. Tree Size")
```

##### We can clearly see from the plot and by using the elbow-method that 6 would be a great value to prune the tree to. Let us try that!

```{r}
# Set seed for reproducibility
set.seed(123)

# Pruning
binary_pruned1 <- prune.tree(binary_tree1, best = 6)

# Predictions on test data, confusion matrix and accuracy calculation
binary_pred2 <- predict(binary_pruned1, newdata = df_test, type = "class")
confusion_matrix2 <- table(Predicted = binary_pred2, Actual = df_test$MRJFLAG)
accuracy2 <- sum(diag(confusion_matrix2)) / sum(confusion_matrix2)
accuracy2

# Plotting Trees
# Unpruned Tree
par(mfrow = c(1, 1), mar = c(1, 1, 4, 1))
plot(binary_tree1, col = 2, lty = 1)
text(binary_tree1, pretty = 0, cex = 0.6)
title("Full Tree", line = 2)

# Pruned Tree
par(mfrow = c(1, 1), mar = c(1, 1, 4, 1))
plot(binary_pruned1, col = 4, lty = 1)
text(binary_pruned1, pretty = 0, cex = 0.6)
title("Pruned Tree", line = 2)
```

##### The accuracy before and after pruning is 74.5% and 68.7%, respectively. Pruning trades off complexity for generalization, it can strip away meaningful parts of the model, leading to reduced accuracy.. We could have also tried bagging and boosting on our binary tree, to optimise it even further.

#### MULTI-CLASS CLASSIFICATION

```{r}
# Class Imbalance Check
table(df_train$MRJYDAYS)
prop.table(table(df_train$MRJYDAYS))
```


```{r}
# Downsampling the training set
df_train_bal <- downSample(
  x = df_train[, -which(names(df_train) == "MRJYDAYS")],
  y = as.factor(df_train$MRJYDAYS),  # Convert to factor
  yname = "MRJYDAYS"
)
# Class Balance Check
print(prop.table(table(df_train_bal$MRJYDAYS)))
```

##### Our response variable is MRJYDAYS which has classes from 1-6, 6 standing for non-user and no past year use. Thus, we eliminate class 6 from our data, to get better insights. We also remove NA values because bagging does not work with NA values in the dataset.

```{r}
# Set seed for reproducibility
set.seed(123)

# Factorization and Filtering of Data
df_train <- subset(df_train, MRJYDAYS %in% 1:5)
df_test <- subset(df_test, MRJYDAYS %in% 1:5)
df_train$MRJYDAYS <- factor(df_train$MRJYDAYS)
df_test$MRJYDAYS <- factor(df_test$MRJYDAYS)

# Remove NA values
df_train <- na.omit(df_train)
df_test <- na.omit(df_test)

# 
multi_model <- randomForest(MRJYDAYS ~ PARCHKHW + PARHLPHW + PRCHORE2 + PRLMTTV2 + 
                             PARLMTSN + PRGDJOB2 + PRPROUD2 + ARGUPAR + 
                             PRMJEVR2 + PRMJMO + IMOTHER + IFATHER + 
                             INCOME + GOVTPROG + POVERTY3,
                            data = df_train,
                            mtry = 15,
                            ntree = 500,
                            importance = TRUE,
                            na.action = na.omit)

#OOB Plot
plot(multi_model$err.rate[,"OOB"], type = "l", col = "purple", lwd = 2,
     xlab = "Number of Trees", ylab = "OOB Error Rate",
     main = "OOB Error vs Trees")

varImpPlot(multi_model)

# Predictions on test data, confusion matrix and accuracy calculation
test_pred <- predict(multi_model, df_test)
test_cm <- table(Predicted = test_pred, Actual = df_test$MRJYDAYS)
test_accuracy <- sum(diag(test_cm)) / sum(test_cm)
cat("Test Set Accuracy:", round(test_accuracy, 3), "\n")
```

##### The graph has a lot of noise. This prompts us to remove some unneccessary variables from the tree model. If we observe the Variable Imporatnce Plots, we realise that IFATHER, PRLMTTV2, PRMJMO, PARHLPHW, PRPROUD2 can be removed from the tree model.

##### Also, we roughly take ntree to be 300, as the graph sort of plateaus after 300.

```{r}
# Set seed for reproducibility
set.seed(123)

# Bagging - Attempt I
multi_bag <- randomForest(MRJYDAYS ~ PARCHKHW + PRCHORE2 + IMOTHER + INCOME + 
                               PRMJEVR2 + PRGDJOB2 + PARLMTSN + POVERTY3 + GOVTPROG +                                    ARGUPAR, data = df_train, mtry = 10, ntree = 300,     
                               importance = TRUE, na.action = na.omit)

# Plot OOB error
plot(multi_bag$err.rate[,"OOB"], type = "l", col = "darkgreen", lwd = 2,
     xlab = "Number of Trees", ylab = "OOB Error Rate",
     main = "Reduced Bagging Model – OOB Error vs Trees")

# Variable importance plot 
varImpPlot(multi_bag)

# Predictions on test data, confusion matrix and accuracy calculations
reduced_pred <- predict(multi_bag, df_test)
reduced_cm   <- table(Predicted = reduced_pred, Actual = df_test$MRJYDAYS)
reduced_acc  <- sum(diag(reduced_cm)) / sum(reduced_cm)
cat("Reduced Bagging Accuracy:", round(reduced_acc, 3), "\n")
```

##### From the graph, an ensemble of 210 trees should be ideal. mtry = 10, which is the total number of predictors. 

```{r}
# Set seed for reproducibility
set.seed(123)

# Bagging - Attempt II
multi_bag_final <- randomForest(MRJYDAYS ~ PARCHKHW + PRCHORE2 + IMOTHER + INCOME +
                                PRMJEVR2 + PRGDJOB2 + PARLMTSN + POVERTY3 +
                                GOVTPROG + ARGUPAR,
                                data = df_train,
                                mtry = 10,          
                                ntree = 210,      
                                importance = TRUE,
                                na.action = na.omit)

# Predictions on test data, confusion matrix and accuracy calculations
final_pred <- predict(multi_bag_final, df_test)
final_cm <- table(Predicted = final_pred, Actual = df_test$MRJYDAYS)
final_accuracy <- sum(diag(final_cm)) / sum(final_cm)
final_accuracy

# Bagging - Attempt III
multi_bag_final1 <- randomForest(MRJYDAYS ~ PARCHKHW + PRCHORE2 + IMOTHER + INCOME +
                                PRMJEVR2 + PRGDJOB2 + PARLMTSN + POVERTY3 +
                                GOVTPROG + ARGUPAR,
                                data = df_train,
                                mtry = 5,          
                                ntree = 210,        
                                importance = TRUE,
                                na.action = na.omit)

# Predictions on test data, confusion matrix and accuracy calculations
final_pred1 <- predict(multi_bag_final1, df_test)
final_cm1 <- table(Predicted = final_pred1, Actual = df_test$MRJYDAYS)
final_accuracy1 <- sum(diag(final_cm1)) / sum(final_cm1)
final_accuracy1

#Importance PLot
importance(multi_bag_final1)
```

##### Since mtry = 10 does not optimise our model enough, we try with mtry = 5, which actually improves our accuracy to 27.7%.

#### REGRESSION

##### For regression we make sure our response variable is numeric and plot MSEs instead of accuracies, as it is preferred for regression models.

```{r}
# Set seed for reproducibility
set.seed(123)

df_train$IRMJFY <- as.numeric(df_train$IRMJFY)
df_test$IRMJFY  <- as.numeric(df_test$IRMJFY)

# Regression Tree
regression_tree <- tree(IRMJFY ~ PARCHKHW + PARHLPHW + PRCHORE2 + PRLMTTV2 + 
                                PARLMTSN + PRGDJOB2 + PRPROUD2 + ARGUPAR + PRMJEVR2 + 
                                PRMJMO + IMOTHER + IFATHER + INCOME + GOVTPROG + 
                                POVERTY3,
                       data = df_train)

pred_tree <- predict(regression_tree, df_test)
mse_tree <- mean((pred_tree - df_test$IRMJFY)^2)
print(paste("Regression Tree Test MSE:", round(mse_tree, 4)))

# Boosting with Cross-Validation
shrinkage_vals <- c(0.001, 0.01, 0.05, 0.1, 0.3)
boost_models <- list()
boost_cv_mses <- numeric(length(shrinkage_vals))
boost_train_mses <- numeric(length(shrinkage_vals))

for (i in seq_along(shrinkage_vals)) {
  model <- gbm(IRMJFY ~ PARCHKHW + PARHLPHW + PRCHORE2 + PRLMTTV2 + 
                         PARLMTSN + PRGDJOB2 + PRPROUD2 + ARGUPAR + PRMJEVR2 + 
                         PRMJMO + IMOTHER + IFATHER + INCOME + GOVTPROG + 
                         POVERTY3,
               data = df_train,
               distribution = "gaussian",
               n.trees = 250,
               interaction.depth = 2,
               shrinkage = shrinkage_vals[i],
               cv.folds = 5,
               verbose = FALSE)

  boost_models[[i]] <- model

  # CV error
  boost_cv_mses[i] <- min(model$cv.error, na.rm = TRUE)

  # Training error
  pred_train <- predict(model, newdata = df_train, n.trees = 250)
  boost_train_mses[i] <- mean((pred_train - df_train$IRMJFY)^2)
}

# Plots
# Training MSE vs Shrinkage
plot(shrinkage_vals, boost_train_mses, type = "b", pch = 19, col = "blue",
     xlab = "Shrinkage (Learning Rate)",
     ylab = "Training MSE",
     main = "Training MSE vs Shrinkage")

# CV MSE vs Shrinkage
plot(shrinkage_vals, boost_cv_mses, type = "b", pch = 19, col = "darkred",
     xlab = "Shrinkage (Learning Rate)",
     ylab = "Cross-Validated MSE",
     main = "Cross-Validated MSE vs Shrinkage")
```

##### We use shrinkage = 0.01 — it gives the lowest CV MSE and best generalization.